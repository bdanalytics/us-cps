---
# Get YAML keywords from myYAML_ref.Rmd
title: "US-CPS: <Predicted attribute> <regression/classification>"
author: "bdanalytics"
pandoc_args: ["+RTS", "-K64M", "-RTS"]

# Choose one:
output:
    html_document:
        keep_md: yes

# output:
#   pdf_document:
#     fig_width: 8
#     highlight: zenburn
#     #keep_md: yes
#     keep_tex: yes
#     number_sections: yes
#     toc: yes
---

**  **    
**Date: `r format(Sys.time(), "(%a) %b %d, %Y")`**    

# Introduction:  

Data: 
Source: 
    Training:   https://courses.edx.org/c4x/MITx/15.071x_2/asset/CPSData.csv  
    New:        <prdct_url>  
Time period: 

```{r set_global_options_wd, echo=FALSE}
setwd("~/Documents/Work/Courses/MIT/Analytics_Edge_15_071x/Assignments/HW1_US_CPS")
```

# Synopsis:

Based on analysis utilizing <> techniques, <conclusion heading>:  

### ![](<filename>.png)

## Potential next steps include:

# Analysis: 
```{r set_global_options}
rm(list=ls())
set.seed(12345)
options(stringsAsFactors=FALSE)
source("~/Dropbox/datascience/R/mydsutils.R")
source("~/Dropbox/datascience/R/myplot.R")
source("~/Dropbox/datascience/R/mypetrinet.R")
# Gather all package requirements here
suppressPackageStartupMessages(require(plyr))

#require(sos); findFn("pinv", maxPages=2, sortby="MaxScore")

# Analysis specific global variables
glb_separate_predict_dataset <- FALSE

script_df <- data.frame(chunk_label="import_data", chunk_step_major=1, chunk_step_minor=0)
print(script_df)
```

## Step ``r script_df[nrow(script_df), "chunk_step_major"]``: import data
```{r import_data, cache=TRUE}
entity_df <- myimport_data(
    url="https://courses.edx.org/c4x/MITx/15.071x_2/asset/CPSData.csv", 
    comment="entity_df", print_diagn=TRUE)
if (glb_separate_predict_dataset) {
    predct_df <- myimport_data(
        url="<prdct_url>", 
        comment="predct_df", print_diagn=TRUE)
} else {
    predct_df <- entity_df[sample(1:nrow(entity_df), nrow(entity_df) / 1000),]
    comment(predct_df) <- "predct_df"
    myprint_df(predct_df)
    str(predct_df)
}         

script_df <- rbind(script_df, 
                   data.frame(chunk_label="inspect_data", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, 
                              chunk_step_minor=1))
print(script_df)
```

### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: inspect data
```{r inspect_data_1, cache=TRUE}
#print(str(entity_df))
#View(entity_df)

# List info gathered for various columns
# <col_name>:   <description>; <notes>

# Create new features that help diagnostics
#   Convert factors to dummy variables
#   Potential Enhancements:
#       One code chunk to cycle thru entity_df & predct_df ?
#           Use with / within ?
#           for (df in c(entity_df, predct_df)) cycles thru column names
#           for (df in list(entity_df, predct_df)) does not change the actual dataframes
#
#       Build splines   require(splines); bsBasis <- bs(training$age, df=3)

entity_df <- mutate(entity_df, 
#     <col_name>_fctr=as.factor(<col_name>),
#     
#     Date.my=as.Date(strptime(Date, "%m/%d/%y %H:%M")),
#     Year=year(Date.my),
#     Month=months(Date.my),
#     Weekday=weekdays(Date.my)
#     
    MetroAreaCode.NA=is.na(MetroAreaCode)
                    )
# 
# predct_df <- mutate(predct_df, 
#                     )

print(summary(entity_df))
print(sapply(names(entity_df), function(col) sum(is.na(entity_df[, col]))))
print(summary(predct_df))
print(sapply(names(predct_df), function(col) sum(is.na(predct_df[, col]))))

#pairs(subset(entity_df, select=-c(col_symbol)))

#   Histogram of predictor in entity_df & predct_df
# Check for predct_df & entity_df features range mismatches

# Other diagnostics:
# print(subset(entity_df, <col1_name> == max(entity_df$<col1_name>, na.rm=TRUE) & 
#                         <col2_name> <= mean(entity_df$<col1_name>, na.rm=TRUE)))

print(Industry_freq_entity_df <- mycreate_tbl_df(entity_df, "Industry"))
print(sort(table(entity_df$State)))
print(Citizenship_freq_entity_df <- as.data.frame(table(entity_df$Citizenship)))
names(Citizenship_freq_entity_df)[1] <- "Citizenship"; print(Citizenship_freq_entity_df)
print(1 - (Citizenship_freq_entity_df[Citizenship_freq_entity_df$Citizenship == "Non-Citizen", "Freq"] * 1.0 / sum(Citizenship_freq_entity_df$Freq)))

print(table(is.na(entity_df$Married), entity_df$Region))
print(table(is.na(entity_df$Married), entity_df$Sex))
print(table(is.na(entity_df$Married), entity_df$Age))

print(prblm_2_4_df <- mycreate_xtab(entity_df, c("Region", "MetroAreaCode.NA")))
prblm_2_4_df[is.na(prblm_2_4_df)] <- 0
print(prblm_2_4_df <- mutate(prblm_2_4_df, MetroCode.NA.ratio = MetroAreaCode.NA.TRUE * 1.0 / (MetroAreaCode.NA.FALSE + MetroAreaCode.NA.TRUE)))

print(prblm_2_5_arr <- sort(tapply(entity_df$MetroAreaCode.NA, entity_df$State, mean, na.rm=TRUE)))

# print(which.min(table(entity_df$<col_name>)))
# print(which.max(table(entity_df$<col_name>)))
# print(which.max(table(entity_df$<col1_name>, entity_df$<col2_name>)[, 2]))
# print(table(entity_df$<col1_name>, entity_df$<col2_name>))
# print(xtabs(~ <col1_name>, entity_df))
# print(xtabs(~ <col1_name> + <col2_name>, entity_df))
print(xtab_entity_df <- mycreate_xtab(entity_df, c("Race", "Hispanic")))
# print(xtab_entity_df <- mutate(xtab_entity_df, 
#             <col3_name>=(<col1_name> * 1.0) / (<col1_name> + <col2_name>))) 

# print(<col2_name>_min_entity_arr <- 
#    sort(tapply(entity_df$<col1_name>, entity_df$<col2_name>, min, na.rm=TRUE)))

# Other plots:
# print(myplot_histogram(entity_df, "<col1_name>"))
# print(myplot_box(df=entity_df, ycol_names="<col1_name>"))
# print(myplot_box(df=entity_df, ycol_names="<col1_name>", xcol_name="<col2_name>"))
# print(myplot_line(subset(entity_df, Symbol %in% c("KO", "PG")), 
#                   "Date.my", "StockPrice", facet_row_colnames="Symbol") + 
#     geom_vline(xintercept=as.numeric(as.Date("2003-03-01"))) +
#     geom_vline(xintercept=as.numeric(as.Date("1983-01-01")))        
#         )
# print(myplot_scatter(entity_df, "<col1_name>", "<col2_name>"))

script_df <- rbind(script_df, 
    data.frame(chunk_label="manage_missing_data", 
        chunk_step_major=max(script_df$chunk_step_major), 
        chunk_step_minor=script_df[which.max(script_df$chunk_step_major), 
                                   "chunk_step_minor"]+1))
print(script_df)
```
### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: manage missing data
```{r manage_missing_data_1, cache=TRUE}

script_df <- rbind(script_df, 
    data.frame(chunk_label="encode_data", 
        chunk_step_major=max(script_df$chunk_step_major), 
        chunk_step_minor=script_df[which.max(script_df$chunk_step_major), 
                                   "chunk_step_minor"]+1))
print(script_df)
```

### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: encode data
```{r encode_data_1, cache=TRUE}
map_metroarea_df <- myimport_data(
    url="https://courses.edx.org/c4x/MITx/15.071x_2/asset/MetroAreaCodes.csv", 
    comment="map_metroarea_df", print_diagn=TRUE)

entity_df <- mymap_codes(entity_df, "MetroAreaCode", "MetroArea", 
    					map_metroarea_df, map_join_col_name="Code")
print(MetroArea_freq_entity_df <- mycreate_tbl_df(entity_df, "MetroArea"))

print(map_metroarea_df[grep("^Atlanta", map_metroarea_df$MetroArea),])
print(nrow(subset(entity_df, MetroAreaCode == "12060")))
print(MetroArea_freq_entity_df[grep("^Atlanta", MetroArea_freq_entity_df$MetroArea),])

print(map_metroarea_df[grep("^Baltimore", map_metroarea_df$MetroArea),])
print(nrow(subset(entity_df, MetroAreaCode == "12580")))
print(MetroArea_freq_entity_df[grep("^Baltimore", MetroArea_freq_entity_df$MetroArea),])

print(map_metroarea_df[grep("^Boston", map_metroarea_df$MetroArea),])
print(nrow(subset(entity_df, MetroAreaCode == "71650")))
print(MetroArea_freq_entity_df[grep("^Boston", MetroArea_freq_entity_df$MetroArea),])

print(map_metroarea_df[grep("^San Francisco", map_metroarea_df$MetroArea),])
print(nrow(subset(entity_df, MetroAreaCode == "41860")))
print(MetroArea_freq_entity_df[grep("^San Francisco", MetroArea_freq_entity_df$MetroArea),])

print(prblm_3_4_arr <- sort(tapply(entity_df$Hispanic, entity_df$MetroArea, mean, na.rm=TRUE)))
print(prblm_3_5_arr <- sort(tapply(entity_df$Race == "Asian", entity_df$MetroArea, mean, na.rm=TRUE)))
print(prblm_3_6_arr <- sort(tapply(entity_df$Education == "No high school diploma", entity_df$MetroArea, mean, na.rm=TRUE), decreasing=TRUE))

map_country_df <- myimport_data(
    url="https://courses.edx.org/c4x/MITx/15.071x_2/asset/CountryCodes.csv", 
    comment="map_metroarea_df", print_diagn=TRUE)

script_df <- rbind(script_df, 
                   data.frame(chunk_label="extract_features", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, 
                              chunk_step_minor=0))
print(script_df)
```

## Step ``r script_df[nrow(script_df), "chunk_step_major"]``: extract features
```{r extract_features, cache=TRUE}

# script_df <- rbind(script_df, 
#                    data.frame(chunk_label="extract_features", 
#                               chunk_step_major=max(script_df$chunk_step_major)+1, 
#                               chunk_step_minor=0))
print(script_df)
```

Null Hypothesis ($\sf{H_{0}}$): mpg is not impacted by am_fctr.  
The variance by am_fctr appears to be independent. 
```{r q1, cache=TRUE}
# print(t.test(subset(cars_df, am_fctr == "automatic")$mpg, 
#              subset(cars_df, am_fctr == "manual")$mpg, 
#              var.equal=FALSE)$conf)
```
We reject the null hypothesis i.e. we have evidence to conclude that am_fctr impacts mpg (95% confidence). Manual transmission is better for miles per gallon versus automatic transmission.

## remove nearZeroVar features (not much variance)
#require(reshape)
#var_features_df <- melt(summaryBy(. ~ factor(0), data=entity_df[, features_lst], 
#                             FUN=var, keep.names=TRUE), 
#                             variable_name=c("feature"))
#names(var_features_df)[2] <- "var"
#print(var_features_df[order(var_features_df$var), ])
# summaryBy ignores factors whereas nearZeroVar inspects factors

# k_fold <- 5
# entity_df[order(entity_df$classe, 
#                   entity_df$user_name, 
#                   entity_df$my.rnorm),"my.cv_ix"] <- 
#     rep(1:k_fold, length.out=nrow(entity_df))
# summaryBy(X ~ my.cv_ix, data=entity_df, FUN=length)
# tapply(entity_df$X, list(entity_df$classe, entity_df$user_name, 
#                            entity_df$my.cv_ix), length)

#require(DAAG)
#entity_df$classe.proper <- as.numeric(entity_df$classe == "A")
#rnorm.glm <- glm(classe.proper ~ rnorm, family=binomial, data=entity_df)
#cv.binary(rnorm.glm, nfolds=k_fold, print.details=TRUE)
#result <- cv.lm(df=entity_df, form.lm=formula(classe ~ rnorm), 
#                    m=k_fold, seed=12345, printit=TRUE)

#plot(mdl_1$finalModel, uniform=TRUE, main="base")
#text(mdl_1$finalModel, use.n=TRUE, all=TRUE, cex=0.8)


```{r print_sessionInfo, echo=FALSE}
sessionInfo()
```